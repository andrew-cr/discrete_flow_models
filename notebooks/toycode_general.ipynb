{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training\n",
    "B = 32 # batch size\n",
    "D = 10 # dimension\n",
    "S = 2 # state space\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, D, S):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(S+1, 16)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(17 * D, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, S*D),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, D = x.shape\n",
    "        x_emb = self.embedding(x) # (B, D, 16)\n",
    "        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1) # (B, D * 17)\n",
    "        return self.net(net_input).reshape(B, D, S) # (B, D, S)\n",
    "\n",
    "model = Model(D, S)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "def sample_p_xt_g_x1(x1, t):\n",
    "    # x1 (B, D)\n",
    "    # t (B,)\n",
    "    # Returns xt (B, D)\n",
    "\n",
    "    # uniform\n",
    "    xt = x1.clone()\n",
    "    uniform_noise = torch.randint(0, S, (B, D))\n",
    "    corrupt_mask = torch.rand((B, D)) < (1 - t[:, None])\n",
    "    xt[corrupt_mask] = uniform_noise[corrupt_mask]\n",
    "\n",
    "    # masking\n",
    "    # xt = x1.clone()\n",
    "    # xt[torch.rand((B,D)) < (1 - t[:, None])] = S-1\n",
    "\n",
    "    return xt\n",
    "\n",
    "\n",
    "for _ in tqdm(range(50000)):\n",
    "    num_ones = torch.randint(0, D+1, (B,))\n",
    "    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long()\n",
    "    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t = torch.rand((B,))\n",
    "    xt = sample_p_xt_g_x1(x1, t)\n",
    "    logits = model(xt, t) # (B, D, S)\n",
    "    loss = F.cross_entropy(logits.transpose(1,2), x1, reduction='mean')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sampling\n",
    "\n",
    "def dt_p_xt_g_xt(x1, t):\n",
    "    # x1 (B, D)\n",
    "    # t float\n",
    "    # returns (B, D, S) for varying x_t value\n",
    "\n",
    "    # uniform\n",
    "    x1_onehot = F.one_hot(x1, num_classes=S) # (B, D, S)\n",
    "    return x1_onehot - (1/S)\n",
    "\n",
    "    # masking\n",
    "    # x1_onehot = F.one_hot(x1, num_classes=S) # (B, D, S)\n",
    "    # M_onehot = F.one_hot(torch.tensor([S-1]), num_classes=S)[None, :, :] # (1, 1, S)\n",
    "    # return x1_onehot - M_onehot\n",
    "\n",
    "def p_xt_g_x1(x1, t):\n",
    "    # x1 (B, D)\n",
    "    # t float\n",
    "    # returns (B, D, S) for varying x_t value\n",
    "\n",
    "    # uniform\n",
    "    x1_onehot = F.one_hot(x1, num_classes=S) # (B, D, S)\n",
    "    return t * x1_onehot + (1-t) * (1/S)\n",
    "\n",
    "    # masking\n",
    "    # x1_onehot = F.one_hot(x1, num_classes=S) # (B, D, S)\n",
    "    # M_onehot = F.one_hot(torch.tensor([S-1]), num_classes=S)[None, :, :] # (1, 1, S)\n",
    "    # return t * x1_onehot + (1-t) * M_onehot\n",
    "\n",
    "\n",
    "def sample_prior(num_samples, D):\n",
    "    # uniform\n",
    "    return torch.randint(0, S, (num_samples, D))\n",
    "\n",
    "    # masking\n",
    "    # return (S-1) * torch.ones((num_samples, D)).long()\n",
    "\n",
    "t = 0.0\n",
    "dt = 0.001\n",
    "num_samples = 1000\n",
    "xt = sample_prior(num_samples, D)\n",
    "\n",
    "while t < 1.0:\n",
    "    logits = model(xt, t * torch.ones((num_samples,))) # (B, D, S)\n",
    "    x1_probs = F.softmax(logits, dim=-1) # (B, D, S)\n",
    "    x1 = Categorical(x1_probs).sample() # (B, D)\n",
    "\n",
    "\n",
    "    # Calculate R_t^*\n",
    "    # For p(x_t | x_1) > 0 and p(j | x_1) > 0\n",
    "    # R_t^*(x_t, j | x_1) = Relu( dtp(j | x_1) - dtp(x_t | x_1)) / (Z_t * p(x_t | x_1))\n",
    "    # For p(x_t | x_1) = 0 or p(j | x_1) = 0 we have R_t^* = 0\n",
    "\n",
    "    # We will ignore issues with diagnoal entries as later on we will set\n",
    "    # diagnoal probabilities such that the row sums to one later on.\n",
    "\n",
    "    dt_p_vals = dt_p_xt_g_xt(x1, t) # (B, D, S)\n",
    "    dt_p_vals_at_xt = dt_p_vals.gather(-1, xt[:, :, None]).squeeze(-1) # (B, D)\n",
    "\n",
    "    # Numerator of R_t^*\n",
    "    R_t_numer = F.relu(dt_p_vals - dt_p_vals_at_xt[:, :, None]) # (B, D, S)\n",
    "\n",
    "    pt_vals = p_xt_g_x1(x1, t) # (B, D, S)\n",
    "    Z_t = torch.count_nonzero(pt_vals, dim=-1) # (B, D)\n",
    "    pt_vals_at_xt = pt_vals.gather(-1, xt[:, :, None]).squeeze(-1) # (B, D)\n",
    "\n",
    "    # Denominator of R_t^*\n",
    "    R_t_denom = Z_t * pt_vals_at_xt # (B, D)\n",
    "\n",
    "    R_t = R_t_numer / R_t_denom[:, :, None] # (B, D, S)\n",
    "\n",
    "    # Set p(x_t | x_1) = 0 or p(j | x_1) = 0 cases to zero\n",
    "    R_t[ (pt_vals_at_xt == 0.0)[:, :, None].repeat(1, 1, S)] = 0.0\n",
    "    R_t[ pt_vals == 0.0] = 0.0\n",
    "\n",
    "\n",
    "    # Calculate the off-diagonal step probabilities\n",
    "    step_probs = (R_t * dt).clamp(max=1.0) # (B, D, S)\n",
    "\n",
    "\n",
    "    # Calculate the on-diagnoal step probabilities\n",
    "    # 1) Zero out the diagonal entries\n",
    "    step_probs.scatter_(-1, xt[:, :, None], 0.0)\n",
    "    # 2) Calculate the diagonal entries such that the probability row sums to 1\n",
    "    step_probs.scatter_(-1, xt[:, :, None], (1.0 - step_probs.sum(dim=-1, keepdim=True)).clamp(min=0.0)) \n",
    "\n",
    "    xt = Categorical(step_probs).sample() # (B, D)\n",
    "\n",
    "    t += dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xt[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(samples)\n",
    "counts = xt.sum(dim=1).float()\n",
    "plt.hist(counts.numpy(), bins=range(D+2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
