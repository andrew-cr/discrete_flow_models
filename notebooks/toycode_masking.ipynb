{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training\n",
    "B = 32 # batch size\n",
    "D = 10 # dimension\n",
    "S = 3 # state space\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, D, S):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(S, 16)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(17 * D, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, (S-1)*D),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, D = x.shape\n",
    "        x_emb = self.embedding(x) # (B, D, 16)\n",
    "        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1) # (B, D * 17)\n",
    "        return self.net(net_input).reshape(B, D, S-1) # (B, D, S-1)\n",
    "\n",
    "model = Model(D, S)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "losses = []\n",
    "for _ in tqdm(range(50000)):\n",
    "    num_ones = torch.randint(0, D+1, (B,))\n",
    "    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long()\n",
    "    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t = torch.rand((B,))\n",
    "    xt = x1.clone()\n",
    "    xt[torch.rand((B,D)) < (1 - t[:, None])] = S-1 # Corrupt with masks, assume 0, 1, ..., S-2 are the valid values and S-1 represents MASK\n",
    "    \n",
    "    # The model outputs logits only over the valid values, we know x1 contains no masks!\n",
    "    logits = model(xt, t) # (B, D, S-1)\n",
    "\n",
    "    x1[xt != S-1] = -1 # don't compute the loss on dimensions that are already revealed\n",
    "    loss = F.cross_entropy(logits.transpose(1,2), x1, reduction='mean', ignore_index=-1)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sampling\n",
    "\n",
    "t = 0.0\n",
    "dt = 0.001\n",
    "num_samples = 1000\n",
    "noise = 10 # noise * dt * D is the average number of dimensions that get re-masked each timestep\n",
    "xt = (S-1) * torch.ones((num_samples, D), dtype=torch.long)\n",
    "\n",
    "while t < 1.0:\n",
    "    logits = model(xt, t * torch.ones((num_samples,))) # (B, D, S-1)\n",
    "    x1_probs = F.softmax(logits, dim=-1) # (B, D, S-1)\n",
    "    x1 = Categorical(x1_probs).sample() # (B, D)\n",
    "    will_unmask = torch.rand((num_samples, D)) < (dt * (1 + noise * t) / (1-t)) # (B, D)\n",
    "    will_unmask = will_unmask * (xt == (S-1)) # (B, D)\n",
    "    will_mask = torch.rand((num_samples, D)) < dt * noise # (B, D)\n",
    "    will_mask = will_mask * (xt != (S-1)) # (B, D)\n",
    "    xt[will_unmask] = x1[will_unmask]\n",
    "\n",
    "    t += dt\n",
    "\n",
    "    if t < 1.0:\n",
    "        xt[will_mask] = S-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xt[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(samples)\n",
    "counts = xt.sum(dim=1).float()\n",
    "plt.hist(counts.numpy(), bins=range(D+2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
