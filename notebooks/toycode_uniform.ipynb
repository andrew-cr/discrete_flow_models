{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training\n",
    "B = 32 # batch size\n",
    "D = 10 # dimension\n",
    "S = 2 # state space\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, D, S):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(S+1, 16)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(17 * D, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, S*D),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, D = x.shape\n",
    "        x_emb = self.embedding(x) # (B, D, 16)\n",
    "        net_input = torch.cat([x_emb, t[:, None, None].repeat(1, D, 1)], dim=-1).reshape(B, -1) # (B, D * 17)\n",
    "        return self.net(net_input).reshape(B, D, S) # (B, D, S)\n",
    "\n",
    "model = Model(D, S)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "losses = []\n",
    "for _ in tqdm(range(50000)):\n",
    "    num_ones = torch.randint(0, D+1, (B,))\n",
    "    x1 = (torch.arange(D)[None, :] < num_ones[:, None]).long()\n",
    "    # x1 e.g. [1, 1, 1, 0, 0, 0, 0, 0, 0, 0] or [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t = torch.rand((B,))\n",
    "    xt = x1.clone()\n",
    "    uniform_noise = torch.randint(0, S, (B, D))\n",
    "    corrupt_mask = torch.rand((B, D)) < (1 - t[:, None])\n",
    "    xt[corrupt_mask] = uniform_noise[corrupt_mask]\n",
    "    logits = model(xt, t) # (B, D, S)\n",
    "    loss = F.cross_entropy(logits.transpose(1,2), x1, reduction='mean', ignore_index=-1)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sampling\n",
    "\n",
    "t = 0.0\n",
    "dt = 0.001\n",
    "num_samples = 1000\n",
    "noise = 1\n",
    "xt = torch.randint(0, S, (num_samples, D))\n",
    "\n",
    "while t < 1.0:\n",
    "    logits = model(xt, t * torch.ones((num_samples,))) # (B, D, S)\n",
    "    x1_probs = F.softmax(logits, dim=-1) # (B, D, S)\n",
    "\n",
    "    x1_probs_at_xt = torch.gather(x1_probs, -1, xt[:, :, None]) # (B, D, 1)\n",
    "\n",
    "    # Don't add noise on the final step\n",
    "    if t + dt < 1.0:\n",
    "        N = noise\n",
    "    else:\n",
    "        N = 0\n",
    "\n",
    "    # Calculate the off-diagonal step probabilities\n",
    "    step_probs = (\n",
    "        dt * ((1 + N + N * (S - 1) * t ) / (1-t)) * x1_probs + \n",
    "        dt * N * x1_probs_at_xt\n",
    "    ).clamp(max=1.0) # (B, D, S)\n",
    "\n",
    "    # Calculate the on-diagnoal step probabilities\n",
    "    # 1) Zero out the diagonal entries\n",
    "    step_probs.scatter_(-1, xt[:, :, None], 0.0)\n",
    "    # 2) Calculate the diagonal entries such that the probability row sums to 1\n",
    "    step_probs.scatter_(-1, xt[:, :, None], (1.0 - step_probs.sum(dim=-1, keepdim=True)).clamp(min=0.0)) \n",
    "\n",
    "    xt = Categorical(step_probs).sample() # (B, D)\n",
    "\n",
    "    t += dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xt[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(samples)\n",
    "counts = xt.sum(dim=1).float()\n",
    "plt.hist(counts.numpy(), bins=range(D+2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(xt_hist.shape)\n",
    "plt.plot(xt_hist[:, 0, 2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
